{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# standard beautiful soup calls\n",
    "def get_page(url):\n",
    "    return requests.get(url)\n",
    "def make_soup(page):\n",
    "    return BeautifulSoup(page.text, 'html')\n",
    "\n",
    "test_url = \"https://www.hockey-reference.com/players/a/acciano01.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating links\n",
    "We want to generate links based on our pre-existing links and years played combination so we can initiate the large scrape - in the order of ~6000 scrapes, so will take 5 hours to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to call active_players_additional.csv and collect the link and generate all possible gamelogs\n",
    "df = pd.read_csv('active_players_additional.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_active = df.at[0, \"years_active\"]\n",
    "link = df.at[0, \"link\"]\n",
    "name = df.at[0, \"name\"]\n",
    "\n",
    "def generate_gamelog_links(link, active_years):\n",
    "    #print(link, active_years)\n",
    "    # first we have to disambiguate the active_years\n",
    "    pattern = r\"(^[0-9]+)-([0-9]+$)\"\n",
    "    result = re.findall(pattern, active_years)\n",
    "    if len(result) == 0:\n",
    "        start = int(active_years)\n",
    "        end = int(active_years)\n",
    "    else:\n",
    "        start = int(result[0][0])\n",
    "        end = int(result[0][1])\n",
    "\n",
    "    # then we generate all the potential links (there could be gaps in between)\n",
    "    links = []\n",
    "    for i in range(start, end+1):\n",
    "        log_link = link[:-5] + \"/gamelog/\" + str(i)\n",
    "        links.append(log_link)\n",
    "\n",
    "    #print(links)\n",
    "    return links\n",
    "\n",
    "# Testing\n",
    "#game_log_links = generate_gamelog_links(link, years_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to scrape a specific gamelog, we will scrape all the information we can so we can potentially use it later\n",
    "\n",
    "# the information we have here is:\n",
    "#   Game #, Date, Age, Team, Opposition, Win/Loss, Goals, Assists, Points, +/-, Penalty Minutes, Even Strength Goals, Powerplay Goals, Shorthanded Goals, Game Winning Goals, Even Strength Assists, Powerplay Assists, Shorthanded Assists, Shots, Shot Percentage, # of shifts, Time on Ice, Hits, Blocks, Face off wins, Face off Losses, Face off %\n",
    "\n",
    "test_link = \"https://www.hockey-reference.com/players/a/acciano01/gamelog/2017\"\n",
    "\n",
    "def scrape_log(link, game_log_dict):\n",
    "    # this function scrapes a particular gamelog based on the given link\n",
    "    # so it scrapes one years worth of data\n",
    "\n",
    "    # generate the soup\n",
    "    soup = make_soup(get_page(link))\n",
    "    log_table = soup.find_all('table', id=\"gamelog\")\n",
    "    if len(log_table) == 1:\n",
    "        log_table = log_table[0]\n",
    "\n",
    "        # identifiers and team outcome\n",
    "        dates = log_table.find_all(attrs={\"data-stat\":\"date_game\", \"aria-label\":None})\n",
    "        age = log_table.find_all(attrs={\"data-stat\":\"age\", \"aria-label\":None})\n",
    "        teams = log_table.find_all(attrs={\"data-stat\":\"team_id\", \"aria-label\":None})\n",
    "        opp = log_table.find_all(attrs={\"data-stat\":\"opp_id\", \"aria-label\":None})\n",
    "        game_result = log_table.find_all(attrs={\"data-stat\":\"game_result\", \"aria-label\":None})\n",
    "\n",
    "        # all situations results\n",
    "        all_goals = log_table.find_all(attrs={\"data-stat\":\"goals\", \"aria-label\":None})\n",
    "        all_assists = log_table.find_all(attrs={\"data-stat\":\"assists\", \"aria-label\":None})\n",
    "        all_points = log_table.find_all(attrs={\"data-stat\":\"points\", \"aria-label\":None})\n",
    "        plus_minus = log_table.find_all(attrs={\"data-stat\":\"plus_minus\", \"aria-label\":None})\n",
    "        pims = log_table.find_all(attrs={\"data-stat\":\"pen_min\", \"aria-label\":None})\n",
    "\n",
    "        # goals in detail\n",
    "        evg = log_table.find_all(attrs={\"data-stat\":\"goals_ev\", \"aria-label\":None})\n",
    "        ppg = log_table.find_all(attrs={\"data-stat\":\"goals_pp\", \"aria-label\":None})\n",
    "        shg = log_table.find_all(attrs={\"data-stat\":\"goals_sh\", \"aria-label\":None})\n",
    "        gwg = log_table.find_all(attrs={\"data-stat\":\"goals_gw\", \"aria-label\":None})\n",
    "\n",
    "        # assists in detail\n",
    "        eva = log_table.find_all(attrs={\"data-stat\":\"assists_ev\", \"aria-label\":None})\n",
    "        ppa = log_table.find_all(attrs={\"data-stat\":\"assists_pp\", \"aria-label\":None})\n",
    "        sha = log_table.find_all(attrs={\"data-stat\":\"assists_sh\", \"aria-label\":None})\n",
    "\n",
    "        # shot data\n",
    "        shots = log_table.find_all(attrs={\"data-stat\":\"shots\", \"aria-label\":None})\n",
    "        shot_pct = log_table.find_all(attrs={\"data-stat\":\"shot_pct\", \"aria-label\":None})\n",
    "\n",
    "        # playing time data\n",
    "        num_shifts = log_table.find_all(attrs={\"data-stat\":\"shifts\", \"aria-label\":None})\n",
    "        toi = log_table.find_all(attrs={\"data-stat\":\"time_on_ice\", \"aria-label\":None})\n",
    "        hits = log_table.find_all(attrs={\"data-stat\":\"hits_all\", \"aria-label\":None})\n",
    "\n",
    "        # Faceoffs\n",
    "        FO_win = log_table.find_all(attrs={\"data-stat\":\"faceoff_wins_all\", \"aria-label\":None})\n",
    "        FO_Loss = log_table.find_all(attrs={\"data-stat\":\"faceoff_losses_all\", \"aria-label\":None})\n",
    "        FO_pct = log_table.find_all(attrs={\"data-stat\":\"faceoff_percentage_all\", \"aria-label\":None})\n",
    "\n",
    "        # now we need to loop through all of this data and add it to the dictionary\n",
    "        for i in range(len(dates)):\n",
    "            # identifiers and team outcome\n",
    "            game_log_dict[\"date\"].append(dates[i].text)\n",
    "            game_log_dict[\"age\"].append(age[i].text)\n",
    "            game_log_dict[\"team\"].append(teams[i].text)\n",
    "            game_log_dict[\"opposition\"].append(opp[i].text)\n",
    "            game_log_dict[\"game_result\"].append(game_result[i].text)\n",
    "\n",
    "            # all situations results\n",
    "            game_log_dict[\"all_goals\"].append(all_goals[i].text)\n",
    "            game_log_dict[\"all_assists\"].append(all_assists[i].text)\n",
    "            game_log_dict[\"all_points\"].append(all_points[i].text)\n",
    "            game_log_dict[\"plus_minus\"].append(plus_minus[i].text)\n",
    "            game_log_dict[\"pims\"].append(pims[i].text)\n",
    "\n",
    "            # goals in detail\n",
    "            game_log_dict[\"evg\"].append(evg[i].text)\n",
    "            game_log_dict[\"ppg\"].append(ppg[i].text)\n",
    "            game_log_dict[\"shg\"].append(shg[i].text)\n",
    "            game_log_dict[\"gwg\"].append(gwg[i].text)\n",
    "\n",
    "            # assists in detail\n",
    "            game_log_dict[\"eva\"].append(eva[i].text)\n",
    "            game_log_dict[\"ppa\"].append(ppa[i].text)\n",
    "            game_log_dict[\"sha\"].append(sha[i].text)\n",
    "\n",
    "            # shot data\n",
    "            game_log_dict[\"shots\"].append(shots[i].text)\n",
    "            game_log_dict[\"shot_pct\"].append(shot_pct[i].text)\n",
    "\n",
    "            # playing time data\n",
    "            game_log_dict[\"num_shifts\"].append(num_shifts[i].text)\n",
    "            game_log_dict[\"toi\"].append(toi[i].text)\n",
    "            game_log_dict[\"hits\"].append(hits[i].text)\n",
    "\n",
    "            # Faceoffs\n",
    "            game_log_dict[\"FO_win\"].append(FO_win[i].text)\n",
    "            game_log_dict[\"FO_Loss\"].append(FO_Loss[i].text)\n",
    "            game_log_dict[\"FO_pct\"].append(FO_pct[i].text)\n",
    "\n",
    "init_dictionary = {\n",
    "    \"date\":[],\n",
    "    \"age\":[],\n",
    "    \"team\":[],\n",
    "    \"opposition\":[],\n",
    "    \"game_result\":[],\n",
    "\n",
    "    \"all_goals\":[],\n",
    "    \"all_assists\":[],\n",
    "    \"all_points\":[],\n",
    "    \"plus_minus\":[],\n",
    "    \"pims\":[],\n",
    "\n",
    "    \"evg\":[],\n",
    "    \"ppg\":[],\n",
    "    \"shg\":[],\n",
    "    \"gwg\":[],\n",
    "\n",
    "    \"eva\":[],\n",
    "    \"ppa\":[],\n",
    "    \"sha\":[],\n",
    "\n",
    "    \"shots\":[],\n",
    "    \"shot_pct\":[],\n",
    "\n",
    "    \"num_shifts\":[],\n",
    "    \"toi\":[],\n",
    "    \"hits\":[],\n",
    "\n",
    "    \"FO_win\":[],\n",
    "    \"FO_Loss\":[],\n",
    "    \"FO_pct\":[]\n",
    "}\n",
    "#scrape_log(test_link, init_dictionary)\n",
    "#print(init_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we put together both to scrape one players entire game logs\n",
    "import time as t\n",
    "\n",
    "# This functions scrapes and stores one player's information\n",
    "def scrape_logs_and_store(df_row, unique_id: str):\n",
    "    # we want to be able to extract information from the file name (act as key)\n",
    "    # start the timer\n",
    "    name = df_row.name\n",
    "    main_link = df_row.link\n",
    "    years_active = df_row.years_active\n",
    "    # get the player links\n",
    "    player_links = generate_gamelog_links(main_link, years_active)\n",
    "    print(\"Time estimate to process\", df_row.name, \"is\", 5*len(player_links), \"seconds, as they have\", len(player_links), \"logs.\")\n",
    "\n",
    "    init_dictionary = {\n",
    "        \"date\":[],\n",
    "        \"age\":[],\n",
    "        \"team\":[],\n",
    "        \"opposition\":[],\n",
    "        \"game_result\":[],\n",
    "\n",
    "        \"all_goals\":[],\n",
    "        \"all_assists\":[],\n",
    "        \"all_points\":[],\n",
    "        \"plus_minus\":[],\n",
    "        \"pims\":[],\n",
    "\n",
    "        \"evg\":[],\n",
    "        \"ppg\":[],\n",
    "        \"shg\":[],\n",
    "        \"gwg\":[],\n",
    "\n",
    "        \"eva\":[],\n",
    "        \"ppa\":[],\n",
    "        \"sha\":[],\n",
    "\n",
    "        \"shots\":[],\n",
    "        \"shot_pct\":[],\n",
    "\n",
    "        \"num_shifts\":[],\n",
    "        \"toi\":[],\n",
    "        \"hits\":[],\n",
    "\n",
    "        \"FO_win\":[],\n",
    "        \"FO_Loss\":[],\n",
    "        \"FO_pct\":[]\n",
    "    }\n",
    "\n",
    "\n",
    "    # scrape every link\n",
    "    for link in player_links:\n",
    "        #print(link)\n",
    "        start = t.time()\n",
    "        scrape_log(link, init_dictionary)\n",
    "        end = t.time()\n",
    "        t.sleep(max(0, 5-(end-start)))\n",
    "        \n",
    "    player_df = pd.DataFrame.from_dict(init_dictionary)\n",
    "    player_df.to_csv(\"data/\"+unique_id+\"_gamelogs.csv\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rename files to update them for proper key information\n",
    "def rename_files(df):\n",
    "    '''\n",
    "    Function used to overwrite file names based on previous naming architecture/convention\n",
    "    Old: Name+number+\"_gamelogs.csv\"\n",
    "    New: name+\"_\"+position+number+\"_gamelogs.csv\"\n",
    "\n",
    "    Reasoning: we want to have unique keys that we can use to recover information. \n",
    "    Obviously we still have issues with same name and same position players.\n",
    "    But we can also recover active_years from looking at the first data in the gamelogs and the last one\n",
    "        -> str(int(date1[:4])+1)+\"-\"+date2 recovers active_years\n",
    "    '''\n",
    "    name_dictionary = {}\n",
    "    unique_id_dict = {}\n",
    "    for row in df.itertuples(name='PlayerInfo'):\n",
    "        if row.position == \"G\":\n",
    "            continue\n",
    "        else:\n",
    "            key = row.name\n",
    "            newKey = row.name+\"_\"+row.position\n",
    "            if key in name_dictionary:\n",
    "                name_dictionary[key] += 1\n",
    "            else:\n",
    "                name_dictionary[key] = 1\n",
    "            \n",
    "            if newKey in unique_id_dict:\n",
    "                unique_id_dict[newKey] += 1\n",
    "            else:\n",
    "                unique_id_dict[newKey] = 1\n",
    "            \n",
    "            unique_id = newKey+str(unique_id_dict[newKey])\n",
    "            old_filepath = \"data/\"+row.name+str(name_dictionary[row.name])+\"_gamelogs.csv\"\n",
    "            if os.path.isfile(old_filepath):\n",
    "                #this is where we rewrite into our new convetion\n",
    "                os.rename(old_filepath, \"data/\"+unique_id+\"_gamelogs.csv\")\n",
    "\n",
    "rename_files(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scrape_hr_gamelogs(df: pd.DataFrame, SKIP_OVERWRITE=True):\n",
    "    name_dictionary = {}\n",
    "    # keeps track of how many times each name has been present\n",
    "    for row in df.itertuples(name='PlayerInfo'):\n",
    "        if row.position == \"G\":\n",
    "            continue\n",
    "        else:\n",
    "            key = row.name+\"_\"+row.position\n",
    "            if key in name_dictionary:\n",
    "                name_dictionary[key] += 1\n",
    "            else:\n",
    "                name_dictionary[key] = 1\n",
    "            \n",
    "            unique_id = key+str(name_dictionary[key])\n",
    "\n",
    "            if SKIP_OVERWRITE:\n",
    "                filepath = \"data/\" + unique_id + \"_gamelogs.csv\"\n",
    "                if os.path.isfile(filepath):\n",
    "                    print(\"Skipped\", row.name, \"as his file already exists, and program is set to not overwrite.\")\n",
    "                else:\n",
    "                    scrape_logs_and_store(row, unique_id=unique_id)\n",
    "            else:\n",
    "                scrape_logs_and_store(row, unique_id=unique_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
